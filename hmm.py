# -*- coding: utf-8 -*-
"""HW2-CSCI544.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_a9QbL-F7uazgoFM3TIatrKR1LYP6_fD
"""

import operator
import json

working_dir = "E:/USC/Spring 2023/Applied Natural Language Processing/HW2/hw2/"
threshold = 2

"""## Task 1: Vocabulary Creation"""

def read_train_data(working_dir):
  f = open(working_dir + "data/train", "r")
  return f.read()

def dev_set_lines(working_dir):
  f = open( working_dir + "data/dev", "r")
  data = f.read()

  # Extracting sentences form data
  lines = data.split("\n\n")

  # Removing the \n at the last line
  lines[-1] = lines[-1][:len(lines[-1])-1]

  return lines

def test_set_lines(working_dir):
  f = open( working_dir + "data/test", "r")
  data = f.read()

  # Extracting sentences form data
  lines = data.split("\n\n")

  # Removing the \n at the last line
  lines[-1] = lines[-1][:len(lines[-1])-1]

  return lines

def word_count():
  """
  Returns the frequency of every word in the corpus
  """

  # Extracting sentences form data
  lines = read_train_data(working_dir).split("\n\n")

  # Removing the \n at the last line
  lines[-1] = lines[-1][:len(lines[-1])-1]

  word = []
  tags = []

  word_dict = {}

  for line in lines:
    words = line.split("\n")
    for x in words:
      tmp = x.split("\t")
      word.append(tmp[1])
      tags.append(tmp[2])
  
  word_dict = dict.fromkeys(word,0)

  for x in word:
    word_dict[x] += 1

  return (word_dict,word,tags,lines)

def removed_words(threshold):
  t = threshold

  removed_words = []
  count_removed_words = 0
  for w in sorted_word_dict:
    if sorted_word_dict[w]<t:
      removed_words.append(w)
      count_removed_words += sorted_word_dict[w]
  
  print("Special token Occurence: ",count_removed_words)
  return (removed_words,count_removed_words)

def create_vocab(count_removed_words,sorted_word_dict):
  vocabulary = []
  vocabulary.append("< unk >\t0\t"+str(count_removed_words))
  i = 1
  for w in sorted_word_dict.keys():
    vocabulary.append(w+"\t"+str(i)+"\t"+str(sorted_word_dict[w]))
    i += 1
  print("Vocabulary Size: ",len(vocabulary))
  return vocabulary

def write_vocab(working_dir, vocabulary):
  flag = True
  with open(working_dir+'vocab.txt', 'w') as fp:
      for item in vocabulary:
        if(flag):
          flag = False
          fp.write("%s" % item)
        else:
          fp.write("\n%s" % item)

def word_tags(sorted_word_dict,word_dict,word,tags,threshold):
  """
  Returns a dict where Word w is the key and all of its corresponding tags as a list in value
  """
  # Stores the word as key and the list of its tags as value
  word_tag_dict = {}

  for x in sorted_word_dict:
    word_tag_dict[x] = []

  for i in range(len(word)):
    if(word_dict[word[i]] < threshold):
      word_tag_dict["< unk >"].append(tags[i])
    else:
      word_tag_dict[word[i]].append(tags[i])
  
  return word_tag_dict

def count_tags(tags):
  """
    freq_tags = Returns each tag and its count
    pos_tags = List of all tags
  """
  # Stores the count of how many times a tag occured in the corpus
  freq_tags = {}

  for t in tags:
    if t in freq_tags.keys():
      freq_tags[t] += 1
    else:
      freq_tags[t] = 1

  # Distinct tags in the corpus are stored as pos_tags
  pos_tags = list(freq_tags.keys())

  return(freq_tags,pos_tags)

def count_word_tag_freq(sorted_word_dict,pos_tags,word_tag_dict):
  # Count the number the times a word and its tag occured in the corpus Ex: word_tag_freq_dict[("NNP","Pierre")] = 6
  word_tag_freq_dict = {}
  for word in sorted_word_dict:
    for t in pos_tags:
      word_tag_freq_dict[(t,word)] = 0

  for x in word_tag_dict:
    w = x
    t = word_tag_dict[x]

    for i in t:
      word_tag_freq_dict[(i,w)] += 1
  
  return word_tag_freq_dict

word_dict,word,tags,lines = word_count()
len(word_dict)
sorted_word_dict = dict( sorted(word_dict.items(), key=operator.itemgetter(1),reverse=True))
removed_words,count_removed_words = removed_words(threshold)
for w in removed_words:
    sorted_word_dict.pop(w)
vocabulary = create_vocab(count_removed_words,sorted_word_dict)
write_vocab(working_dir,vocabulary)

# Adding < unk > in sorted_word_dict
sorted_word_dict['< unk >'] = count_removed_words
word_tag_dict = word_tags(sorted_word_dict,word_dict,word,tags,threshold)
freq_tags,pos_tags = count_tags(tags)
word_tag_freq_dict = count_word_tag_freq(sorted_word_dict,pos_tags,word_tag_dict)

"""## Task 2: Model Learning

### Transition Probilities
"""

def transition_probabilities(lines,freq_tags):

  # Stores the count of how many times the tag t2 occured after tag t1.
  t2_after_t1 = {}
  for t2 in pos_tags:
    for t1 in pos_tags:
      t2_after_t1[(t2, t1)] = 0
    t2_after_t1[(t2, '<s>')] = 0

  for line in lines:
    tags_in_line = []
    words = line.split("\n")
    for x in words:
      tmp = x.split("\t")
      tags_in_line.append(tmp[2])
    
    for i in range(len(tags_in_line)):
      if(i==0):
        t2_after_t1[(tags_in_line[i], '<s>')] += 1
      else:
        t2_after_t1[(tags_in_line[i], tags_in_line[i-1])] += 1
  
  # Transition Probabilities
  transition = {}
  for x in t2_after_t1:
    if(x[1]=='<s>'):
      # Wait for the reply on the piazza post
      transition[str((x[1],x[0]))] = t2_after_t1[x]/len(lines)
      continue
    else:
      transition[str((x[1],x[0]))] = t2_after_t1[x]/freq_tags[x[1]]
  
  return transition

"""### Emission Probabilities"""

def emission_probabilities(word_tag_freq_dict,freq_tags):
  # Emission
  emission = {}

  for x in word_tag_freq_dict:
      # Calculate P(w,t) and P(t)-> freq_tags[t]
      emission[str(x)] = word_tag_freq_dict[x]/freq_tags[x[0]]
  
  return emission

"""### Write in file"""

def write_transition_emission(working_dir,transition,emission):
  hmm = {}
  hmm['transition'] = transition
  hmm['emission'] = emission

  with open( working_dir + "hmm.json", "w") as outfile:
      json.dump(hmm, outfile)

transition = transition_probabilities(lines,freq_tags)
emission = emission_probabilities(word_tag_freq_dict,freq_tags)
print("Numeber of transition parameters:",len(transition))
print("Number of emission parameters:",len(emission))
write_transition_emission(working_dir,transition,emission)

"""## Task 3: Greedy Decoding with HMM"""

def greedy_on_dev(lines,sorted_word_dict,pos_tags,emission,transition):

  count_true = 0
  count_false = 0
  for line in lines:
    words = line.split("\n")
    prev_tag = '<s>'
    for x in words:
      tmp = x.split("\t")

      max_prob = 0
      predicted_tag = ""
      for t in pos_tags:
        if tmp[1] not in sorted_word_dict.keys():
          x = emission[str((t,"< unk >"))]*transition[str((prev_tag,t))]
        else:
          x = emission[str((t,tmp[1]))]*transition[str((prev_tag,t))]
        if x>=max_prob:
          max_prob = x
          predicted_tag = t
      prev_tag = predicted_tag

      if(predicted_tag==tmp[2]):
        count_true += 1
      else:
        #print(tmp[1],tmp[2],predicted_tag)
        count_false += 1
      #print(tmp[1],tmp[2],predicted_tag)
  
  #print(count_true)
  #print(count_false)
  greedy_acc = count_true/(count_true+count_false)
  print("Greedy Algorithm Accuracy:",greedy_acc)

  return greedy_acc

def greedy_on_test(lines, sorted_word_dict, pos_tags, emission, transition, working_dir):
  """
  Runs the Greedy Algorithm on Test set and write the POS tags in greedy.out
  """
  output = ""
  for line in lines:
    words = line.split("\n")
    prev_tag = '<s>'
    for x in words:
      tmp = x.split("\t")

      max_prob = 0
      predicted_tag = ""
      for t in pos_tags:
        if tmp[1] not in sorted_word_dict.keys():
          x = emission[str((t,"< unk >"))]*transition[str((prev_tag,t))]
        else:
          x = emission[str((t,tmp[1]))]*transition[str((prev_tag,t))]
        if x>=max_prob:
          max_prob = x
          predicted_tag = t
      prev_tag = predicted_tag
      output += tmp[0]+"\t"+tmp[1]+"\t"+predicted_tag+"\n"
    output += "\n"

  output = output[:len(output)-1]
  f = open( working_dir + "greedy.out", "w")
  f.write(output)

lines = dev_set_lines(working_dir)
greedy_acc = greedy_on_dev(lines,sorted_word_dict,pos_tags,emission,transition)
lines = test_set_lines(working_dir)
greedy_on_test(lines,sorted_word_dict,pos_tags,emission,transition, working_dir)

"""## Task 4: Viterbi Decoding with HMM"""

def viterbi_on_dev(lines,sorted_word_dict,pos_tags,emission,transition):
  """
  Returns the accuracy of Viterbi Algorithm on Dev Set
  """
  count_true = 0
  count_false = 0

  for line in lines:
      
    words = line.split("\n")
    original_tags_of_words = []
    mat = [[0 for _ in range(len(words))] for _ in range(len(pos_tags))]
    backtrack_mat = [[-1 for _ in range(len(words))] for _ in range(len(pos_tags))]

    for i in range(len(words)):
      tmp = words[i].split("\t")
      original_tags_of_words.append(tmp[2])
      if(i==0):
        for j in range(len(pos_tags)):
          if tmp[1] not in sorted_word_dict.keys():
            # <unk>
            mat[j][i] = emission[str((pos_tags[j],"< unk >"))]*transition[str(('<s>',pos_tags[j]))]
          else:
            mat[j][i] = emission[str((pos_tags[j],tmp[1]))]*transition[str(('<s>',pos_tags[j]))]
            # Known
      else:
        for j in range(len(pos_tags)):
          maximum = 0  
          ind_max = -1    
          for j1 in range(len(pos_tags)):
            if tmp[1] not in sorted_word_dict.keys():
              x = mat[j1][i-1]*emission[str((pos_tags[j],"< unk >"))]*transition[str((pos_tags[j1],pos_tags[j]))]
            else:
              x = mat[j1][i-1]*emission[str((pos_tags[j],tmp[1]))]*transition[str((pos_tags[j1],pos_tags[j]))]
            if x>maximum:
              maximum = x
              ind_max = j1
          mat[j][i] = maximum
          backtrack_mat[j][i] = ind_max
          #print(mat[j][i])

    start_index = -1
    maxi = -1
    last_column = len(words) - 1
    for i in range(len(pos_tags)):
      if mat[i][last_column] > maxi:
        maxi = mat[i][last_column]
        start_index = i

    # df = pd.DataFrame(mat)
    # df2 = pd.DataFrame(backtrack_mat)
    # print(df)
    # print(df2)
    # print(start_index)
    answers = []
    answers.append(start_index)
    for i in range(len(words)-1,0,-1):
      prev_max = backtrack_mat[start_index][i]
      answers.append(prev_max)
      start_index = prev_max

    answers.reverse()

    predicted_tags_of_words = []
    for i in answers:
      predicted_tags_of_words.append(pos_tags[i])

    for i in range(len(predicted_tags_of_words)):
      if predicted_tags_of_words[i] == original_tags_of_words[i]:
        count_true += 1
      else:
        count_false += 1
    # print(predicted_tags_of_words)
    # print(original_tags_of_words)
  viterbi_acc = count_true/(count_true+count_false)
  #print(count_true)
  #print(count_false)
  print("Viterbi Algorithm Accuracy:",viterbi_acc)
  return viterbi_acc

def viterbi_on_test(lines, sorted_word_dict, pos_tags, emission, transition, working_dir):
  """
  Runs the Viterbi Algorithm on Test set and write the POS tags in viterbi.out
  """
  output = ""
  for line in lines:
    words = line[0]  
    words = line.split("\n")
    mat = [[0 for _ in range(len(words))] for _ in range(len(pos_tags))]
    backtrack_mat = [[-1 for _ in range(len(words))] for _ in range(len(pos_tags))]
    w = []

    for i in range(len(words)):
      tmp = words[i].split("\t")
      w.append(tmp[1])
      if(i==0):
        for j in range(len(pos_tags)):
          if tmp[1] not in sorted_word_dict.keys():
            # <unk>
            mat[j][i] = emission[str((pos_tags[j],"< unk >"))]*transition[str(('<s>',pos_tags[j]))]
          else:
            mat[j][i] = emission[str((pos_tags[j],tmp[1]))]*transition[str(('<s>',pos_tags[j]))]
            # Known
      else:
        for j in range(len(pos_tags)):
          maximum = 0  
          ind_max = -1    
          for j1 in range(len(pos_tags)):
            if tmp[1] not in sorted_word_dict.keys():
              x = mat[j1][i-1]*emission[str((pos_tags[j],"< unk >"))]*transition[str((pos_tags[j1],pos_tags[j]))]
            else:
              x = mat[j1][i-1]*emission[str((pos_tags[j],tmp[1]))]*transition[str((pos_tags[j1],pos_tags[j]))]
            if x>maximum:
              maximum = x
              ind_max = j1
          mat[j][i] = maximum
          backtrack_mat[j][i] = ind_max
          #print(mat[j][i])

    start_index = -1
    maxi = -1
    last_column = len(words) - 1
    for i in range(len(pos_tags)):
      if mat[i][last_column] > maxi:
        maxi = mat[i][last_column]
        start_index = i

    answers = []
    answers.append(start_index)
    for i in range(len(words)-1,0,-1):
      prev_max = backtrack_mat[start_index][i]
      answers.append(prev_max)
      start_index = prev_max

    answers.reverse()

    predicted_tags_of_words = []
    for i in answers:
      predicted_tags_of_words.append(pos_tags[i])

    for i in range(len(w)):
      output += str(i+1) + "\t" + w[i] + "\t" + predicted_tags_of_words[i] + "\n"
    output+="\n"

  output = output[:len(output)-1]
  f = open(working_dir + "viterbi.out", "w")
  f.write(output) 
  #print(output)

lines = dev_set_lines(working_dir)
viterbi_acc = viterbi_on_dev(lines,sorted_word_dict,pos_tags,emission,transition)
lines = test_set_lines(working_dir)
viterbi_on_test(lines,sorted_word_dict,pos_tags,emission,transition, working_dir)

"""## Results"""

print("Threshold:",threshold)
print("Greedy Accuracy:", greedy_acc)
print("Viterbi Accuracy", viterbi_acc)

"""Threshold: 2 <br>
Greedy Accuracy: 0.9339217412421832 <br>
Viterbi Accuracy 0.9473013174670634 <br><br>


Threshold: 3 <br>
Greedy Accuracy: 0.9288294578349827 <br>
Viterbi Accuracy 0.9432866856900006 <br><br>

Threshold: 4 <br>
Greedy Accuracy: 0.9252853500091069 <br>
Viterbi Accuracy 0.939932305263797 <br><br>

Threshold: 5 <br>
Greedy Accuracy: 0.9213390200959262 <br>
Viterbi Accuracy 0.9369573796369376 <br><br>
"""